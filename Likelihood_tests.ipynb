{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chack for updated files\\n,\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext line_profiler\n",
    "\n",
    "#import packages\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import glob, json\n",
    "import pickle\n",
    "import os as os_pack\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "#%matplotlib inline\\n\",\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]\n",
    "import healpy as hp\n",
    "import os, glob, json, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "import enterprise\n",
    "from enterprise.pulsar import Pulsar\n",
    "import enterprise.signals.parameter as parameter\n",
    "from enterprise.signals import utils\n",
    "from enterprise.signals import signal_base\n",
    "from enterprise.signals import selections\n",
    "from enterprise.signals.selections import Selection\n",
    "from enterprise.signals import white_signals\n",
    "from enterprise.signals import gp_signals\n",
    "from enterprise.signals import deterministic_signals\n",
    "import enterprise.constants as const\n",
    "from enterprise_extensions import blocks\n",
    "from enterprise_extensions import models as ee_models\n",
    "from enterprise_extensions import model_utils as ee_model_utils\n",
    "from enterprise_extensions import model_orfs\n",
    "from enterprise_extensions.frequentist import optimal_statistic as opt_stat\n",
    "from enterprise_extensions import sampler as ee_sampler\n",
    "from enterprise.signals.signal_base import LogLikelihood\n",
    "import enterprise_wavelets as models\n",
    "from enterprise.signals.deterministic_signals import Deterministic\n",
    "from enterprise.signals.parameter import function\n",
    "from la_forge.core import Core\n",
    "from la_forge.diagnostics import plot_chains\n",
    "from la_forge import rednoise\n",
    "import la_forge\n",
    "import corner\n",
    "import h5py\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc\n",
    "import re\n",
    "#style\n",
    "import cProfile\n",
    "    \n",
    "#import Fast_Burst_likelihood as FB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0471a32",
   "metadata": {},
   "source": [
    "# Testing likelihood in notebook with bad samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabbe7f",
   "metadata": {},
   "source": [
    "## Copying PTA generation from QuickBurst_MCMC\n",
    "\n",
    "# 7/7 - Incomplete PTA generation/likelihood testing for bad samples from previous chain. \n",
    "A bit more complicated, since we need to make sure the generated PTA matches the one we are using in our likelihood calls, as well as ensuring that the jumps match (will need possible if statements for each kind of jump (and copy likelihood calls out of QuickBurst_MCMC for each jump).\n",
    "# I may be overcomplicating this. \n",
    "\n",
    "Template: \n",
    "\n",
    "- create pta (complicated part -> needs to match chain setup)\n",
    "- create likelihood objects for each chain\n",
    "- calculate likelihood from each object using bad samples\n",
    "- check which steps are being taken while going through samples\n",
    "\n",
    "\n",
    "# 7/17/23 Edit: I was indeed OVERCOMPLICATING things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736dc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 2 wavelet dataset\n",
    "# parDir =  '/home/reyna/BayesHopperBurst/BenceData/2_Wavelet_Sim/par/'\n",
    "# timDir =  '/home/reyna/BayesHopperBurst/BenceData/2_Wavelet_Sim/2_wavelet_tims/'\n",
    "# psrlist = sorted(glob.glob(parDir+'*.par'))\n",
    "# for _ in range(len(psrlist)):\n",
    "#     psrlist[_] = re.sub(parDir, '', psrlist[_])\n",
    "#     psrlist[_] = re.sub('_NANOGrav_12yv2.gls.par', '', psrlist[_])\n",
    "# parfiles = sorted(glob.glob(parDir+'*.par'))\n",
    "# timfiles = sorted(glob.glob(timDir+'*.tim'))\n",
    "# psrs_sim = []\n",
    "# for p, t in zip(parfiles, timfiles):\n",
    "#     psr = Pulsar(p, t, ephem=None, clk=None)\n",
    "#     psrs_sim.append(psr)\n",
    "\n",
    "########## 12.5 year Astro4Cast dataset\n",
    "with open(\"/home/reyna/BayesHopperBurst/QuickBurst/12p5_year_A4Cast/quickburst_test_12p5yrAstro4Cast_parabolic_3Mpc_60M_psrs.pkl\", 'rb') as f:\n",
    "    psrs_sim = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in pickle and noise files\n",
    "################# 2 wavelet dataset\n",
    "#noise_file_sim = \"/home/reyna/BayesHopperBurst/BenceData/with_burst/params_simulated.json\"\n",
    "################# 12.5 year Astro4cast\n",
    "noise_file_sim = \"/home/reyna/BayesHopperBurst/QuickBurst/12p5_year_A4Cast/quickburst_test_12p5yrAstro4Cast_parabolic_3Mpc_60M/channelized_12p5yr_v3_full_noisedict.json\"\n",
    "\n",
    "#psrlist = np.loadtxt('/home/reyna/15yr_v1p0/15yr_v1-20211001T235643Z-001/15yr_v1/psrlist_15yr_pint.txt', dtype = str)\n",
    "with open(noise_file_sim, 'r') as h:\n",
    "    noise_params_sim = json.load(h)\n",
    "#Temporary to get code to not crash\n",
    "psrs_sim = psrs_sim[0:]\n",
    "psrlist = [psr.name for psr in psrs_sim]\n",
    "print(psrlist)\n",
    "for i in range(len(psrs_sim)):\n",
    "    print((max(psrs_sim[i].toas) - min(psrs_sim[i].toas))/(3.17*10**(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dict_sim = {} #new noise dict to conver the simulated noise file into a readable formate\n",
    "for k,v in noise_params_sim.items():\n",
    "    if '_equad' in k:\n",
    "        noise_dict_sim.update({k.split('_equad')[0] + '_t2equad': v})\n",
    "    else:\n",
    "        noise_dict_sim.update({k : v})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b583c89",
   "metadata": {},
   "source": [
    "### Set number of chains , tau scan and glitch tau scan file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 12.5 year Astr4cast\n",
    "ts_file = \"/home/reyna/BayesHopperBurst/QuickBurst/12p5_year_A4Cast/tauscan_12p5yr-A4Cast_like_real_Allnoise_3MPc.pkl\"\n",
    "glitch_ts_file = \"/home/reyna/BayesHopperBurst/QuickBurst/12p5_year_A4Cast/glitch_tauscan_12p5yr-A4Cast_like_real_Allnoise_3MPc.pkl\"\n",
    "\n",
    "############## 2 wavelet dataset\n",
    "# ts_file = \"/home/reyna/BayesHopperBurst/BenceData/2_Wavelet_Sim/tauscan_2_wavelet_dataset.pkl\"\n",
    "# glitch_ts_file = \"/home/reyna/BayesHopperBurst/BenceData/2_Wavelet_Sim/tauscan_2_wavelet_dataset.pkl.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95c100",
   "metadata": {},
   "source": [
    "### Copy run settings from run you'd like to test against enterprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting dataset max time and reference time\n",
    "maximum = 0\n",
    "minimum = np.inf\n",
    "for psr in psrs_sim:\n",
    "    if psr.toas.max() > maximum:\n",
    "        maximum = psr.toas.max()\n",
    "    if psr.toas.min() < minimum:\n",
    "        minimum = psr.toas.min()\n",
    "\n",
    "\n",
    "#Sets reference time\n",
    "tref = minimum\n",
    "\n",
    "t0_max = (maximum - minimum)/365/24/3600\n",
    "print(t0_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c14d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Various parameters used to generate PTA (copy from run you want to test with)\n",
    "#(for keeping PTA the same in testing setting)\n",
    "max_n_wavelet=2\n",
    "min_n_wavelet=0\n",
    "n_wavelet_start=1\n",
    "RJ_weight=2\n",
    "glitch_RJ_weight=2\n",
    "regular_weight=4\n",
    "noise_jump_weight=2\n",
    "PT_swap_weight=2\n",
    "tau_scan_proposal_weight=2\n",
    "glitch_tau_scan_proposal_weight=2\n",
    "tau_scan_file=ts_file\n",
    "glitch_tau_scan_file=glitch_ts_file\n",
    "#gwb_log_amp_range=[-18,-15]\n",
    "rn_log_amp_range=[-18,-15]\n",
    "wavelet_log_amp_range=[-10.0,-5]\n",
    "per_psr_rn_log_amp_range=[-18,-11]\n",
    "prior_recovery=False\n",
    "#gwb_amp_prior='log-uniform'\n",
    "rn_amp_prior='log-uniform'\n",
    "wavelet_amp_prior='uniform'\n",
    "per_psr_rn_amp_prior='log-uniform'\n",
    "#gwb_on_prior=0.975\n",
    "max_n_glitch=2\n",
    "n_glitch_start=1\n",
    "glitch_log_amp_range=[-10.0,-5]\n",
    "glitch_amp_prior='uniform'\n",
    "t0_max=t0_max\n",
    "tref = tref\n",
    "draw_from_prior_weight=0\n",
    "de_weight=0\n",
    "vary_white_noise=False\n",
    "include_rn=True\n",
    "vary_rn=True\n",
    "include_equad=True\n",
    "include_ecorr=True\n",
    "wn_backend_selection=True\n",
    "noisedict=noise_dict_sim\n",
    "include_per_psr_rn=True\n",
    "vary_per_psr_rn=True\n",
    "ent_lnlike_test=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_wavelet=3\n",
    "min_n_wavelet=0\n",
    "n_wavelet_start=1\n",
    "RJ_weight=2\n",
    "glitch_RJ_weight=2\n",
    "regular_weight=4\n",
    "noise_jump_weight=2\n",
    "PT_swap_weight=2\n",
    "tau_scan_proposal_weight=2\n",
    "glitch_tau_scan_proposal_weight=2\n",
    "tau_scan_file=ts_file\n",
    "glitch_tau_scan_file=glitch_ts_file\n",
    "#gwb_log_amp_range=[-18,-15]\n",
    "rn_log_amp_range=[-18,-15]\n",
    "wavelet_log_amp_range=[-10.0,-5]\n",
    "per_psr_rn_log_amp_range=[-18,-11]\n",
    "prior_recovery=False\n",
    "#gwb_amp_prior='log-uniform'\n",
    "rn_amp_prior='log-uniform'\n",
    "wavelet_amp_prior='uniform'\n",
    "per_psr_rn_amp_prior='log-uniform'\n",
    "#gwb_on_prior=0.975\n",
    "max_n_glitch=3\n",
    "n_glitch_start=1\n",
    "glitch_log_amp_range=[-10.0,-5]\n",
    "glitch_amp_prior='uniform'\n",
    "t0_max=t0_max\n",
    "tref = tref\n",
    "draw_from_prior_weight=0\n",
    "de_weight=0\n",
    "vary_white_noise=True\n",
    "include_rn=True \n",
    "vary_rn=True\n",
    "include_equad=True\n",
    "include_ecorr=False\n",
    "wn_backend_selection=True\n",
    "noisedict=noise_dict_sim\n",
    "include_per_psr_rn=True\n",
    "vary_per_psr_rn=True\n",
    "#resume_from=savefile\n",
    "#per_psr_rn_start_file=RN_start_file\n",
    "save_every_n=100\n",
    "n_fast_to_slow=10\n",
    "thin = 1\n",
    "ent_lnlike_test = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fdd23",
   "metadata": {},
   "source": [
    "### Creating PTA objects for QB and enterprise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac8fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import QuickBurst_MCMC_testing as QB_MCMC\n",
    "import QB_FastPrior\n",
    "\n",
    "#PTA generation w/ \n",
    "pta, ent_ptas, QB_FP, QB_FPI, glitch_indx, wavelet_indx, per_puls_indx, rn_indx, num_per_puls_param_list = QB_MCMC.get_pta(psrs_sim, vary_white_noise=vary_white_noise, include_equad=include_equad,\n",
    "                                                                                                    include_ecorr = include_ecorr, wn_backend_selection=wn_backend_selection,\n",
    "                                                                                                    noisedict=noisedict, include_rn=include_rn, vary_rn=vary_rn,\n",
    "                                                                                                    include_per_psr_rn=include_per_psr_rn, vary_per_psr_rn=vary_per_psr_rn,\n",
    "                                                                                                    max_n_wavelet=max_n_wavelet, efac_start = None, rn_amp_prior=rn_amp_prior,\n",
    "                                                                                                    rn_log_amp_range=rn_log_amp_range, rn_params=[-13.0,1.0], per_psr_rn_amp_prior=per_psr_rn_amp_prior,\n",
    "                                                                                                    per_psr_rn_log_amp_range=per_psr_rn_log_amp_range, gwb_amp_prior='uniform',\n",
    "                                                                                                    gwb_log_amp_range=[-18, -11], wavelet_amp_prior=wavelet_amp_prior,\n",
    "                                                                                                    wavelet_log_amp_range=wavelet_log_amp_range, prior_recovery=prior_recovery, ent_lnlike_test = ent_lnlike_test,\n",
    "                                                                                                    max_n_glitch=max_n_glitch, glitch_amp_prior=glitch_amp_prior, glitch_log_amp_range=glitch_log_amp_range,\n",
    "                                                                                                    t0_min=0.0, t0_max=t0_max, f0_min=3.5e-9, f0_max=1e-7,\n",
    "                                                                                                    TF_prior=None, tref=tref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ent_ptas[n_wavelet_start][n_glitch].params for n_glitch in range(max_n_glitch+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e79d19",
   "metadata": {},
   "source": [
    "### Loading in chain and step_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb13a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/Testing/2_wavelet_sim/convergencetest/enterprise_comparison/3w3g_allnoise_alljumps_on\"\n",
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/Testing/2_wavelet_sim/bugtesting/enterprise_comparison_take2/2w2g_allnoisevaried_alljumps_newLike_oldMCMC_enterprisetest_take2\"\n",
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/Testing/2_wavelet_sim/bugtesting/enterprise_comparison_take2/2w2g_allnoisevaried_CovCalc_alljumps_oldLike_oldMCMC_enterprisetest_take5\"\n",
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/Testing/2_wavelet_sim/bugtesting/enterprise_comparison_take2/2w2g_allnoisevaried_CovCalc_SigmaNotAppend_alljumps_oldLike_oldMCMC_enterprisetest\"\n",
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/Testing/2_wavelet_sim/bugtesting/enterprise_comparison_take2/2w2g_allnoisevaried_CovCalc_rever_logdet_alljumps_oldLike_oldMCMC_enterprisetest\"\n",
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/12p5_year_A4Cast/chains/12p5yr_A4Cast_2g2w_nointWN_dictchange_oldlike_oldMCMC_enterprisetest\"\n",
    "#filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/Testing/2_wavelet_sim/bugtesting/enterprise_comparison_take2/3w3g_allnoisevaried_CovCalc_UnJit_alljumps_oldMCMC_enterprisetest\"\n",
    "filepath = \"/home/reyna/BayesHopperBurst/QuickBurst/12p5_year_A4Cast/chains/12p5yr_A4Cast_2g2w_nointWN_CovCalc_UnJit_oldMCMC_enterprisetest\"\n",
    "chain_filepath = filepath + '.h5df'\n",
    "with h5py.File(chain_filepath, 'r+') as f:\n",
    "    n_chain = 2\n",
    "    samples_array = f['samples_cold'][()]\n",
    "    log_likelihood = f['log_likelihood'][()]\n",
    "    ent_lnlikelihood = f['ent_lnlikelihood'][()]\n",
    "    acc_frac = f['acc_fraction'][()]\n",
    "    param_names = f['par_names'][()]\n",
    "\n",
    "print(acc_frac)\n",
    "#print(ent_lnlikelihood)\n",
    "samples = samples_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wavelet = int(samples[0,1,0])\n",
    "n_glitch = int(samples[0,1,1])\n",
    "temp_entlike1 = ent_ptas[n_wavelet][n_glitch].get_lnlikelihood(QB_MCMC.remove_params(samples[0,1,2:], 0, 0, wavelet_indx, glitch_indx, n_glitch = n_glitch, n_wavelet = n_wavelet, max_n_glitch = max_n_glitch, max_n_wavelet = max_n_wavelet, params_slice = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e73f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ent_ptas[n_wavelet][n_glitch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_ptas[n_wavelet][n_glitch].items()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = dict((k,v) for k,v in zip(pta.param_names, samples[0, 0, 2:]))\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TNT = pta.get_TNT(d0)\n",
    "phiinvs = pta.get_phiinv(d0, logdet = True, method = 'partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56952e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(TNT[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "phiinv, logdet = phiinvs[1]\n",
    "np.shape(phiinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c7201",
   "metadata": {},
   "source": [
    "### Getting samples that don't agree with enterprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d8a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting bad samples from samples_array\n",
    "bad_samples = []\n",
    "idxs = []\n",
    "n_chain = [n for n in range(len(acc_frac[6]))]\n",
    "starting_indx = 0\n",
    "ending_indx = int(len(log_likelihood[0])/1)\n",
    "for n in n_chain:\n",
    "    print(n)\n",
    "    np.max(np.exp(log_likelihood[n][:] - ent_lnlikelihood[n][:]))\n",
    "    temp_array = []\n",
    "    temp_idxs = []\n",
    "    temp_samp_array = []\n",
    "    for i in range(starting_indx, ending_indx):\n",
    "        if (np.exp(log_likelihood[n][i] - ent_lnlikelihood[n][i]) > 1+1e-6 ):#and np.exp(log_likelihood[n][i] - ent_lnlikelihood[n][i]) < 1+1e-11):\n",
    "            temp_array.append((log_likelihood[n][i] - ent_lnlikelihood[n][i]))\n",
    "            temp_samp_array.append(samples[n, i, :])\n",
    "            temp_idxs.append(i)\n",
    "        if (np.exp(log_likelihood[n][i] - ent_lnlikelihood[n][i]) < 1-1e-6 ):#and np.exp(log_likelihood[n][i] - ent_lnlikelihood[n][i]) > 1-1e-11):\n",
    "            temp_array.append((log_likelihood[n][i] - ent_lnlikelihood[n][i]))\n",
    "            temp_samp_array.append(samples[n, i, :])\n",
    "            temp_idxs.append(i)\n",
    "    idxs.append(temp_idxs)\n",
    "    bad_samples.append(temp_samp_array)\n",
    "    print(len(temp_array))\n",
    "    print(temp_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in n_chain:\n",
    "    print(i)\n",
    "    for ii in idxs[i]:\n",
    "        print(ii)\n",
    "        print(log_likelihood[i][ii])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af96db23",
   "metadata": {},
   "source": [
    "### Recreate likelihood object for every bad step, likelihood should match enterprise exactly since we recreate QuickBurst likelihood object from scratch.\n",
    "- Print out various likelihood parts for each bad sample\n",
    "- Perform new run and save relevant likelihood parts for comparison to notebook likelihoods (which match enterprise)\n",
    "- Compare likelihood attributes at relevant stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf3088",
   "metadata": {},
   "source": [
    "# OLD CODE IN THIS CELL\n",
    "### Load in likelihood attributes from QuickBurst MCMC run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51cff5",
   "metadata": {},
   "source": [
    "### Load in likelihood_attributes and step_array from chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filepath = filepath + '.pkl'\n",
    "with open(pkl_filepath, 'rb') as f:\n",
    "    likelihood_attributes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383d786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_array_filepath = filepath + '_step_array.txt'\n",
    "print(step_array_filepath)\n",
    "step_array = np.loadtxt(step_array_filepath, dtype = str)\n",
    "# step_array = []\n",
    "# #Getting rid of delimiters that make two or more columns\n",
    "# with open(step_array_filepath) as fin:\n",
    "#     full_array = fin.read()\n",
    "#     print(np.shape(full_array))\n",
    "#     step_array_temp = full_array.split('\\n')\n",
    "#     for i in range(len(step_array_temp)):\n",
    "#         step_array.append(step_array_temp[i].split(', '))\n",
    "# print(step_array[0])\n",
    "    \n",
    "# # step_array = np.loadtxt(step_array_filepath, dtype = 'str')\n",
    "# print(np.shape(step_array[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(step_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3f26fa",
   "metadata": {},
   "source": [
    "## Get attributes for notebook object at each bad step\n",
    "### 7/27/23: Switching to using dictionaries for purposes of appending to when resuming runs, and accessing later on via key value pairs.  ~ Jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935c33b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import QuickBurst_lnlike as QB\n",
    "sanity_check = False\n",
    "#Generate dictionary with all empty lists for every parameter we care to look at\n",
    "notebook_attr_dict = {}\n",
    "\n",
    "attributes = []\n",
    "\n",
    "##Generate new likehoods for notebook starting here.\n",
    "bad_QB_likelihoods = []\n",
    "bad_ent_lnlikelihoods = []\n",
    "\n",
    "for n in n_chain:\n",
    "    print('Chain {}'.format(n))\n",
    "    temp_likelihoods = []\n",
    "    temp_ent_like = []\n",
    "    \n",
    "    #Make list of attributes to look at based on json file\n",
    "    attributes.append([key for key in likelihood_attributes['chain_{}'.format(n)].keys()])\n",
    "    \n",
    "    #Make empty dictionary for all chains\n",
    "    notebook_attr_dict['chain_{}'.format(n)] = {}\n",
    "    \n",
    "    #Make temp dict that has keys for individual chain\n",
    "    temp_chain_dict = {key: [] for key in likelihood_attributes['chain_{}'.format(n)].keys()}\n",
    "    \n",
    "    #Skip chain if no bad samples\n",
    "    if (len(bad_samples[n]) == 0):\n",
    "        bad_QB_likelihoods.append(temp_likelihoods)\n",
    "        bad_ent_lnlikelihoods.append(temp_ent_like)\n",
    "        continue\n",
    "        \n",
    "    #Remake object during each sample, run bad sample through script and check against enterprise\n",
    "    for i in range(len(bad_samples[n])):\n",
    "        #Setup object with previous sample\n",
    "        QB_logl = QB.QuickBurst(pta = pta, psrs = psrs_sim, params = dict(zip(pta.param_names, samples[n, idxs[n][i], 2:])), \n",
    "                                Npsr = len(psrs_sim), tref=tref, Nglitch = int(samples[n, idxs[n][i], 1]), Nwavelet = int(samples[n, idxs[n][i], 0]), \n",
    "                                Nglitch_max = max_n_glitch ,Nwavelet_max = max_n_wavelet, rn_vary = vary_rn, wn_vary = vary_white_noise, prior_recovery = prior_recovery)\n",
    "        #Setup next step\n",
    "        n_wavelet_new = int(samples[n, idxs[n][i], 0])# get_n_wavelet(samples, j, 0)\n",
    "        n_glitch_new = int(samples[n, idxs[n][i], 1])# get_n_glitch(samples, j, 0)\n",
    "        bad_step = samples[n, idxs[n][i], 2:]\n",
    "\n",
    "\n",
    "        temp_likelihoods.append(QB_logl.get_lnlikelihood(bad_step, vary_white_noise = True, vary_red_noise = True))\n",
    "        #print('Enterprise PTA TNr: ', ent_ptas[n_wavelet_new][n_glitch_new].get_TNr(bad_step))\n",
    "        temp_ent_like.append(ent_ptas[n_wavelet_new][n_glitch_new].get_lnlikelihood(QB_MCMC.remove_params(bad_step, 0, 0, wavelet_indx, glitch_indx, n_glitch = n_glitch_new, n_wavelet = n_wavelet_new, max_n_glitch = max_n_glitch, max_n_wavelet = max_n_wavelet, params_slice = True)))\n",
    "\n",
    "        #above should be fine, but occasionaly gets NAN error\n",
    "        #print('Ent pta params for above glitch/wavelet:', ent_ptas[n_wavelet_new][n_glitch_new].params)\n",
    "\n",
    "        #Compare attributes for various steps\n",
    "        for jj in temp_chain_dict.keys():\n",
    "            #Print statement to check we are comparing apples to apples\n",
    "\n",
    "            #Save current object attr to dictionary\n",
    "            #print('Saving {0} to notebook dict for chain {1}'.format(attributes_to_load[jj], n), '\\n')\n",
    "            temp_chain_dict[jj].append(np.copy(getattr(QB_logl, jj)))\n",
    "\n",
    "    #Save temporary chain dict to first relevant empty dictionary\n",
    "    notebook_attr_dict['chain_{}'.format(n)] = temp_chain_dict\n",
    "    print(notebook_attr_dict['chain_{}'.format(n)].keys())\n",
    "    \n",
    "    bad_QB_likelihoods.append(temp_likelihoods)\n",
    "    bad_ent_lnlikelihoods.append(temp_ent_like)\n",
    "    #first_sample = strip_samples(samples, j, 0, n_wavelet, max_n_wavelet, n_glitch, max_n_glitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0feb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pta.get_TNT(dict(zip(pta.param_names, samples[0, idxs[0][0], 2:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba56590",
   "metadata": {},
   "source": [
    "### Turn ndarray elements into lists to match type of object that is likelihood_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6f81b",
   "metadata": {},
   "source": [
    "#### For Notebook QB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(n_chain)):\n",
    "    print(n)\n",
    "    #Skip chain if no bad samples\n",
    "    if (len(bad_samples[n]) == 0):\n",
    "        bad_QB_likelihoods.append(temp_likelihoods)\n",
    "        bad_ent_lnlikelihoods.append(temp_ent_like)\n",
    "        continue\n",
    "    for jj in attributes[n]:\n",
    "        #Check if attribute is ndarray\n",
    "        if str(type(notebook_attr_dict['chain_{}'.format(n)][jj][0])) == \"<class 'numpy.ndarray'>\":\n",
    "            #If ndarray, go through all samples and change each element to list\n",
    "            for kk in range(len(notebook_attr_dict['chain_{}'.format(n)][jj])):\n",
    "                #Save over ndarray with list type object\n",
    "                temp_list = notebook_attr_dict['chain_{}'.format(n)][jj][kk].tolist()\n",
    "                notebook_attr_dict['chain_{}'.format(n)][jj][kk] = temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c787cf",
   "metadata": {},
   "source": [
    "#### For MCMC QB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8434c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(n_chain)):\n",
    "    print(n)\n",
    "    #Skip chain if no bad samples\n",
    "    if (len(bad_samples[n]) == 0):\n",
    "        bad_QB_likelihoods.append(temp_likelihoods)\n",
    "        bad_ent_lnlikelihoods.append(temp_ent_like)\n",
    "        continue\n",
    "    for jj in attributes[n]:\n",
    "        print(jj)\n",
    "        #Check if attribute is ndarray\n",
    "        for kk in range(len(notebook_attr_dict['chain_{}'.format(n)][jj])):\n",
    "            #print(str(type(likelihood_attributes['chain_{}'.format(n)][jj][kk])))\n",
    "            if str(type(likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][kk]])) == \"<class 'numpy.ndarray'>\":\n",
    "                #If ndarray, go through all samples and change each element to list\n",
    "                #Save over ndarray with list type object\n",
    "                #temp_list = likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][kk]].tolist()\n",
    "                likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][kk]] = likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][kk]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba4ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(likelihood_attributes['chain_0']['params'][idxs[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac77f7b",
   "metadata": {},
   "source": [
    "# Compare notebook object properties at each step vs MCMC object properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944178ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_lnlikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95fd15e",
   "metadata": {},
   "source": [
    "### Calculate likelihood difference between resampled bad likelihood points to enterprise likelihoods. These should be 0 (or close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961177f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_like_resample_diff = []\n",
    "notebook_enterprise_diff = []\n",
    "QB_like_diff = []\n",
    "enterprise_like_diff = []\n",
    "for n in range(len(n_chain)):\n",
    "    print(n)\n",
    "    if (len(bad_samples[n]) == 0):\n",
    "        continue\n",
    "        \n",
    "    temp_like_diff = []\n",
    "    temp_note_diff = []\n",
    "    temp_QB_diff = []\n",
    "    temp_ent_diff = []\n",
    "    \n",
    "    for i in range(len(bad_samples[n])):\n",
    "        temp_like_diff.append(bad_QB_likelihoods[n][i] - ent_lnlikelihood[n][idxs[n][i]])\n",
    "        temp_note_diff.append(bad_QB_likelihoods[n][i] - bad_ent_lnlikelihoods[n][i])\n",
    "        temp_QB_diff.append(bad_QB_likelihoods[n][i] - log_likelihood[n][idxs[n][i]])\n",
    "        temp_ent_diff.append(bad_ent_lnlikelihoods[n][i] - ent_lnlikelihood[n][idxs[n][i]])\n",
    "        \n",
    "    ent_like_resample_diff.append(temp_like_diff)\n",
    "    notebook_enterprise_diff.append(temp_note_diff)\n",
    "    QB_like_diff.append(temp_QB_diff)\n",
    "    enterprise_like_diff.append(temp_ent_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574f609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Notebook QB - MCMC ent\n",
    "ent_like_resample_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fef2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Notebook QB - MCMC QB\n",
    "QB_like_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15167a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Notebook ent - MCMC ent (should always be 0)\n",
    "enterprise_like_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(len(enterprise_like_diff[0])):\n",
    "    print('Step taken: ', step_array[idxs[0][ii]], '\\n')\n",
    "    print('Notebook ent, MCMC ent: ', bad_ent_lnlikelihoods[0][ii], ent_lnlikelihood[0][ii], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b83aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab46988",
   "metadata": {},
   "source": [
    "### Get difference of all properties of the QB class (notebook - MCMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3efa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notebook_attr_dict['chain_{}'.format(0)]['Nvecs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268abe5e",
   "metadata": {},
   "source": [
    "### Parameter subtraction (Notebook - MCMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae76ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# like_property_diff = []\n",
    "# for n in n_chain:\n",
    "#     temp_like_diff = []\n",
    "#     like_property_diff.append(temp_like_diff)\n",
    "\n",
    "like_property_diff = {}\n",
    "\n",
    "#Loop through each chain\n",
    "for n in n_chain:\n",
    "    temp_diff_dict = {}\n",
    "    print(n)\n",
    "    \n",
    "    temp_diff_dict = {key: [] for key in likelihood_attributes['chain_{}'.format(n)].keys()}\n",
    "    #print(temp_diff_dict.keys())\n",
    "    #Check if there are no bad samples for each chain\n",
    "    if (len(bad_samples[n]) == 0):\n",
    "        continue\n",
    "        \n",
    "    #Loop through all bad samples in chains\n",
    "    for i in range(len(bad_samples[n])):\n",
    "        for jj in attributes[n]:   \n",
    "            \n",
    "            #If dimensions are greater than 1, take difference for every element for each attribute for each chain sample\n",
    "            #Also check if object type is list. If either is true, do element wise subtraction.\n",
    "            if len(np.shape(notebook_attr_dict['chain_{}'.format(n)][jj][i])) > 1 or str(type(notebook_attr_dict['chain_{}'.format(n)][jj][i])) == \"<class 'list'>\" or str(type(notebook_attr_dict['chain_{}'.format(n)][jj][i])) == \"<class 'dict'>\":\n",
    "                #print('Loop! Ms and Ns: ', jj)\n",
    "                \n",
    "#                 print(type(notebook_attr_dict['chain_{}'.format(n)][jj][i]))\n",
    "#                 print(type(likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][i]].tolist()))\n",
    "\n",
    "                \n",
    "                #Store each attribute to temp variable\n",
    "                a = notebook_attr_dict['chain_{}'.format(n)][jj][i]\n",
    "                b = likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][i]]\n",
    "            \n",
    "                #Hardcoded, but if property is a dictionary\n",
    "                if jj == 'params':\n",
    "                    #print('yeah:', a.keys())\n",
    "                    #print('YEAH: ', b.keys())\n",
    "                    #Create difference in param key value pairs into new temp dict\n",
    "                    temp_var_dict = {key: a[key] - b[key] for key in a.keys()}\n",
    "                    #Append temp dict to temp dictionary for set of params at sample i\n",
    "                    temp_diff_dict[jj].append(temp_var_dict)\n",
    "                    \n",
    "                else:\n",
    "                    #Otherwise, do element wise subtraction for elements in 2D arrays\n",
    "#                     print(notebook_attr_dict['chain_{}'.format(n)][jj][i])\n",
    "#                     print(likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][i]])\n",
    "                    temp_diff_dict[jj].append(np.subtract(list(notebook_attr_dict['chain_{}'.format(n)][jj][i]), list(likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][i]])))\n",
    "    \n",
    "            #handles cases of 0d objects     \n",
    "            else:\n",
    "                #Otherwise, do element wise subtraction for elements in 2D arrays\n",
    "                temp_diff_dict[jj].append(np.subtract(notebook_attr_dict['chain_{}'.format(n)][jj][i], likelihood_attributes['chain_{}'.format(n)][jj][idxs[n][i]]))\n",
    "\n",
    "    like_property_diff['chain_{}'.format(n)] = temp_diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd312d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(like_property_diff['chain_0']['wavelet_prm'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d443d",
   "metadata": {},
   "source": [
    "### Printing out ratio of elements for each param between QB notebook object and MCMC object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8bd9e3",
   "metadata": {},
   "source": [
    "## Calculate all likelihood properties ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(likelihood_attributes['chain_0'][attributes[0][2]][idxs[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_attr_dict['chain_{}'.format(1)][attributes[1][1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb598467",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(likelihood_attributes['chain_{}'.format(chain_n)][attributes[chain_n][param_check]][idxs[chain_n][check_i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5eff18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "like_property_diff['chain_{}'.format(chain_n)][attributes[chain_n][param_check]][check_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37271bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_n = 0\n",
    "check_i = 0\n",
    "print(samples[chain_n, idxs[chain_n][check_i], 2:])\n",
    "print(samples[chain_n, idxs[chain_n][check_i]-1, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b15824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary_ratios = {}\n",
    "\n",
    "#Loop through each chain\n",
    "for n in n_chain:\n",
    "    temp_ratio_dict = {}\n",
    "    print(n)\n",
    "    \n",
    "    #Create temp ratio dictionary for each chain\n",
    "    temp_ratio_dict = {key: [] for key in likelihood_attributes['chain_{}'.format(n)].keys()}\n",
    "    \n",
    "    #Check if there are no bad samples for each chain\n",
    "    if (len(bad_samples[n]) == 0):\n",
    "        continue\n",
    "        \n",
    "    print(attributes[n])\n",
    "        \n",
    "    #Loop through all bad samples in chains\n",
    "    for i in range(len(bad_samples[n])):\n",
    "        for jj in attributes[n]:    \n",
    "            #If dimensions are greater than 1, take difference for every element for each attribute for each chain sample\n",
    "            #Also check if object type is list. If either is true, do element wise subtraction.\n",
    "            if len(np.shape(notebook_attr_dict['chain_{}'.format(n)][jj][i])) > 1 or str(type(notebook_attr_dict['chain_{}'.format(n)][jj][i])) == \"<class 'list'>\" or str(type(notebook_attr_dict['chain_{}'.format(n)][jj][i])) == \"<class 'dict'>\":\n",
    "                #Store each attribute to temp variable\n",
    "                a = like_property_diff['chain_{}'.format(n)][jj][i]\n",
    "                b = notebook_attr_dict['chain_{}'.format(n)][jj][i]\n",
    "\n",
    "                #Hardcoded, but if property is a dictionary\n",
    "                if jj == 'params':\n",
    "                    #print('params print: ', jj)\n",
    "                    #Create difference in param key value pairs into new temp dict\n",
    "                    temp_var_dict = {key: a[key]/b[key] for key in a.keys()}\n",
    "\n",
    "                    #Append temp dict to temp dictionary for set of params at sample i\n",
    "                    temp_ratio_dict[jj].append(temp_var_dict)\n",
    "\n",
    "                elif jj == 'wavelet_prm':\n",
    "                    #print('wavelet_prm print: ', jj)\n",
    "                    #do stuff\n",
    "                    #print('types: ', type(like_property_diff['chain_{}'.format(n)][jj][i]), type(notebook_attr_dict['chain_{}'.format(n)][jj][i]))\n",
    "                    temp_wave_prm = np.array(notebook_attr_dict['chain_{}'.format(n)][jj][i])\n",
    "                    temp_ratio_dict[jj].append(np.divide(like_property_diff['chain_{}'.format(n)][jj][i], temp_wave_prm))\n",
    "                    \n",
    "                else:\n",
    "                    #print('else print: ', jj)\n",
    "                    #Otherwise, do element wise subtraction for elements in 2D arrays\n",
    "                    temp_ratio_dict[jj].append(np.divide(like_property_diff['chain_{}'.format(n)][jj][i], notebook_attr_dict['chain_{}'.format(n)][jj][i]))\n",
    "            else:\n",
    "                #Otherwise, do element wise subtraction for elements in 0D arrays\n",
    "                temp_ratio_dict[jj].append(np.divide(like_property_diff['chain_{}'.format(n)][jj][i], notebook_attr_dict['chain_{}'.format(n)][jj][i]))\n",
    "                    \n",
    "    dictionary_ratios['chain_{}'.format(n)] = temp_ratio_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60198248",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb372b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_n = 0\n",
    "param_check = 4\n",
    "check_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs[chain_n][check_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d468ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_array[idxs[chain_n][check_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c534c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_attributes['chain_{}'.format(chain_n)][attributes[chain_n][param_check]][idxs[chain_n][check_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d248d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likelihood_attributes['chain_{}'.format(chain_n)][attributes[chain_n][param_check]][idxs[chain_n][check_index]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd96561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_ratios['chain_{}'.format(chain_n)][attributes[chain_n][param_check]][check_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f12c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMrow = 7\n",
    "like_property_diff[2][0][0][MMrow]/notebook_attr_dict[chain_var_names[2][0]][0][MMrow]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668796d0",
   "metadata": {},
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e355a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = 500\n",
    "\n",
    "rn = True\n",
    "wn = False\n",
    "\n",
    "log_L_Ent = []\n",
    "log_L_Fast = []\n",
    "accept = []\n",
    "#amplitudes = []\n",
    "d0 = parameter.sample(pta_sim.params)\n",
    "d0_previous = d0.copy()\n",
    "x0 = np.array([d0[par.name] for par in pta_sim.params])\n",
    "FB1.get_lnlikelihood(x0, vary_white_noise = wn, vary_red_noise = rn)\n",
    "FB1.save_values(accept_new_step=True)\n",
    "for n in range(NN):\n",
    "    #amplitudes.append(i*0.5e-6)\n",
    "    if 0.5 < np.random.uniform():\n",
    "        #print('normal step')\n",
    "        d0 = parameter.sample(pta_sim.params)\n",
    "        x0 = np.array([d0[par.name] for par in pta_sim.params])\n",
    "        log_L_Ent.append(pta_sim.get_lnlikelihood(x0))\n",
    "        log_L_Fast.append(FB1.get_lnlikelihood(x0, vary_white_noise = wn, vary_red_noise = rn))#, no_step = True))\n",
    "    else:\n",
    "        #print('fast step')\n",
    "        d0_2 = parameter.sample(pta_sim.params)\n",
    "        for i in range(len(psrs_sim)):\n",
    "            for k,par in enumerate(pta_sim.param_names):\n",
    "                if psrs_sim[i].name in par: #set perpulsar terms to match\n",
    "                    if 'ecorr' in par or 'efac' in par or 'equad' in par or 'log10_A' in par or 'gamma' in par:\n",
    "                        d0_2[pta_sim.param_names[k]] = d0_previous[pta_sim.param_names[k]]\n",
    "                elif 'gw_gamma' in par or 'gw_log10_A' in par:#match common process\n",
    "                    d0_2[pta_sim.param_names[k]] = d0_previous[pta_sim.param_names[k]]\n",
    "        for nn in range(N_glitches):\n",
    "            d0_2['Glitch_'+str(nn)+'_log10_f0'] = d0_previous['Glitch_'+str(nn)+'_log10_f0']\n",
    "            d0_2['Glitch_'+str(nn)+'_psr_idx'] = d0_previous['Glitch_'+str(nn)+'_psr_idx']\n",
    "            d0_2['Glitch_'+str(nn)+'_t0'] = d0_previous['Glitch_'+str(nn)+'_t0']\n",
    "            d0_2['Glitch_'+str(nn)+'_tau'] = d0_previous['Glitch_'+str(nn)+'_tau']\n",
    "        for ww in range(N_wavelets):\n",
    "            d0_2['wavelet_'+str(ww)+'_log10_f0'] = d0_previous['wavelet_'+str(ww)+'_log10_f0']\n",
    "            d0_2['wavelet_'+str(ww)+'_t0'] = d0_previous['wavelet_'+str(ww)+'_t0']\n",
    "            d0_2['wavelet_'+str(ww)+'_tau'] = d0_previous['wavelet_'+str(ww)+'_tau']\n",
    "        #print('d0: ',d0)\n",
    "        #print('d0_2 modified: ',d0_2)\n",
    "        #d0_2 = d0\n",
    "        x0_2 = np.array([d0_2[par.name] for par in pta_sim.params])\n",
    "        log_L_Ent.append(pta_sim.get_lnlikelihood(x0_2))\n",
    "        log_L_Fast.append(FB1.get_lnlikelihood(x0_2, vary_white_noise = False, vary_red_noise = False))\n",
    "    if 0.5 < np.random.uniform():\n",
    "        FB1.save_values(accept_new_step=False)\n",
    "        accept.append('No')\n",
    "        d0 = d0_previous.copy()\n",
    "    else:\n",
    "        FB1.save_values(accept_new_step=True)\n",
    "        accept.append('Yes')\n",
    "        d0_previous = d0.copy()\n",
    "    #print('run ',n)\n",
    "log_L_Ent = np.array(log_L_Ent)\n",
    "log_L_Fast = np.array(log_L_Fast)\n",
    "accept = np.array(accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "pta_sim.param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Glitch_4_log10_f0': -7.227727184731241, 'Glitch_4_log10_h': -9.184254001093874, 'Glitch_4_phase0': 5.126291443469657, 'Glitch_4_psr_idx': 11.06653403004687, 'Glitch_4_t0': 6.781043649900989, 'Glitch_4_tau': 1.012983761027325, 'JPSR00_efac': 5.7865257197768125, 'JPSR00_log10_t2equad': -5.458430039323788, 'JPSR01_efac': 7.408289003479412, 'JPSR01_log10_t2equad': -7.386667862884638, 'JPSR02_efac': 5.694954690794321, 'JPSR02_log10_t2equad': -7.1875982388319795, 'JPSR03_efac': 4.356071771472992, 'JPSR03_log10_t2equad': -5.212509749153512, 'JPSR04_efac': 8.003935924228779, 'JPSR04_log10_t2equad': -8.043245424082997, 'JPSR05_efac': 3.4396696312250334, 'JPSR05_log10_t2equad': -7.09399898317015\n",
    "'Glitch_4_log10_f0': -7.227727184731241, 'Glitch_4_log10_h': -9.120508561286098, 'Glitch_4_phase0': 0.6447336458972177, 'Glitch_4_psr_idx': 11.06653403004687, 'Glitch_4_t0': 6.781043649900989, 'Glitch_4_tau': 1.012983761027325, 'JPSR00_efac': 5.7865257197768125, 'JPSR00_log10_t2equad': -5.458430039323788, 'JPSR01_efac': 7.408289003479412, 'JPSR01_log10_t2equad': -7.386667862884638, 'JPSR02_efac': 5.694954690794321, 'JPSR02_log10_t2equad': -7.1875982388319795, 'JPSR03_efac': 4.356071771472992, 'JPSR03_log10_t2equad': -5.212509749153512, 'JPSR04_efac': 8.003935924228779, 'JPSR04_log10_t2equad': -8.043245424082997, 'JPSR05_efac': 3.4396696312250334, 'JPSR05_log10_t2equad': -7.09399898317015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d58bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_L_Ent[10:30])\n",
    "print(log_L_Fast[10:30])\n",
    "print(accept[10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec76873",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_L_Ent, log_L_Ent, ls='--', marker='.', color='xkcd:blue', label = 'Ent')\n",
    "plt.plot(log_L_Ent, log_L_Fast, ls='', marker='.', color='xkcd:green', label = 'FastB')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel(\"ent log(like)\")\n",
    "plt.ylabel(\"fast log(likelihood)\")\n",
    "plt.title('lnlikelihood comparisons multi glitch on sim')\n",
    "#plt.savefig('/home/reyna/BayesHopperBurst/testing plots/simulated_multi_transient_like_comp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344caabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.exp(log_L_Fast - log_L_Ent))\n",
    "temp_array = []\n",
    "idxs = []\n",
    "for i in range(len(log_L_Fast)):\n",
    "    if np.exp(log_L_Fast[i] - log_L_Ent[i]) > 1e2:\n",
    "        temp_array.append(np.exp(log_L_Fast[i] - log_L_Ent[i]))\n",
    "        idxs.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe146f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(log_L_Fast - log_L_Ent), ls='', marker='.', color='xkcd:blue')\n",
    "#plt.gca().axhline(0.1, ls='--', color='xkcd:green')\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"delta log(likelihood)\")\n",
    "#plt.ylim(bottom = 1-1e-9, top = 1+1e-9)\n",
    "plt.title('dif for sim random samples')\n",
    "#plt.savefig('/home/reyna/BayesHopperBurst/testing plots/simulated_Multi_transient_precentDif.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ae9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_L_Fast - log_L_Ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_L_Fast/log_L_Ent - 1, ls='', marker='.', color='xkcd:blue')\n",
    "#plt.gca().axhline(0.1, ls='--', color='xkcd:green')\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"delta log(likelihood)\")\n",
    "plt.title('%dif for randome samples')\n",
    "#plt.savefig('/home/reyna/BayesHopperBurst/testing plots/simulated_Multi_transient_precentDif.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(amplitudes, log_L_Ent, ls='--', marker='.', color='xkcd:green', label = 'Ent')\n",
    "plt.plot(amplitudes, log_L_Fast, ls='', marker='.', color='xkcd:blue', label = 'FastB')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"amp\")\n",
    "plt.ylabel(\"log(likelihood)\")\n",
    "plt.title('lnlikelihood vs amplitude simulated single transient')\n",
    "#plt.savefig('/home/reyna/BayesHopperBurst/testing plots/simulated_transient_like_VS_amp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(amplitudes, log_L_Fast-log_L_Ent, ls='', marker='.', color='xkcd:blue')\n",
    "#plt.gca().axhline(0.1, ls='--', color='xkcd:green')\n",
    "plt.xlabel(\"amp\")\n",
    "plt.ylabel(\"delta log(likelihood)\")\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(amplitudes, log_L_Fast/log_L_Ent - 1., ls='', marker='.', color='xkcd:blue')\n",
    "#plt.gca().axhline(0.1, ls='--', color='xkcd:green')\n",
    "plt.xlabel(\"amp\")\n",
    "plt.ylabel(\"precent diff log(likelihood)\")\n",
    "plt.title('%dif vs amplitude simulated single transient')\n",
    "plt.savefig('/home/reyna/BayesHopperBurst/testing plots/simulated_transient_precentDif_VS_amp.png')\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae338a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, psr in enumerate(psrs_sim):\n",
    "    plt.figure(i)\n",
    "    plt.plot(psr.toas/86400, psr.residuals, ls='', marker='.')\n",
    "    plt.plot(psr.toas/86400, pta_sim.get_delay(d0)[i],color='xkcd:red')\n",
    "    \n",
    "    f0 = 10**(x0[0])\n",
    "    tau = (365.25*24*3600)*x0[5]\n",
    "    t0 = (365.25*24*3600)*x0[4]\n",
    "    \n",
    "    Cosine = np.exp(-1*((FB1.toas[i] - t0)/tau)**2)*np.cos(2*np.pi*f0*(FB1.toas[i] - t0))\n",
    "    Sine = np.exp(-1*((FB1.toas[i] - t0)/tau)**2)*np.sin(2*np.pi*f0*(FB1.toas[i] - t0))\n",
    "    print(FB1.sigma)\n",
    "    fast_delay = FB1.sigma[0]*Cosine + FB1.sigma[1]*Sine\n",
    "    plt.plot(psr.toas/86400, fast_delay- pta_sim.get_delay(d0)[i] , ls='--',color='xkcd:green') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, psr in enumerate(psrs_sim):\n",
    "    plt.figure(i)\n",
    "    #plt.plot(psr.toas/86400, psr.residuals, ls='', marker='.')\n",
    "    #plt.plot(psr.toas/86400, pta_sim.get_delay(d0)[i],color='xkcd:red')\n",
    "    \n",
    "    f0 = 10**(x0[0])\n",
    "    tau = (365.25*24*3600)*x0[5]\n",
    "    t0 = (365.25*24*3600)*x0[4]\n",
    "    \n",
    "    Sigma0 = 10**(x0[1])*np.cos(x0[2]) #10**(x0[1]), x0[2]\n",
    "    Sigma1 = -10**(x0[1])*np.sin(x0[2]) #10**(x0[1]), x0[2]\n",
    "    \n",
    "    Cosine = np.exp(-1*((FB1.toas[i] - t0)/tau)**2)*np.cos(2*np.pi*f0*(FB1.toas[i] - t0))\n",
    "    Sine = np.exp(-1*((FB1.toas[i] - t0)/tau)**2)*np.sin(2*np.pi*f0*(FB1.toas[i] - t0))\n",
    "    print(FB1.sigma)\n",
    "    fast_delay = Sigma0*Cosine + Sigma1*Sine #FB1.sigma[0]*Cosine + FB1.sigma[1]*Sine\n",
    "    plt.plot(psr.toas/86400, fast_delay - pta_sim.get_delay(d0)[i], ls='-',color='xkcd:green') \n",
    "    #plt.plot(psr.toas/86400, FB1.sigma[0]*Cosine , ls='--',color='xkcd:blue') \n",
    "    #plt.plot(psr.toas/86400, FB1.sigma[1]*Sine , ls='--',color='xkcd:red') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496990b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
